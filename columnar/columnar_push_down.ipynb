{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614fe103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "commits = spark.read.parquet(\"gs://insinyur-data/github4\")\n",
    "commits.createOrReplaceTempView('commits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f83860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- committer: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- time_sec: long (nullable = true)\n",
      " |    |-- tz_offset: long (nullable = true)\n",
      " |    |-- date: timestamp (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commits.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da04750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+--------------+\n",
      "|   name|               date|             message|     repo_name|\n",
      "+-------+-------------------+--------------------+--------------+\n",
      "|       |2006-01-10 13:39:07|MIPS: Au1550: Fix...|torvalds/linux|\n",
      "|       |2006-02-28 04:55:02|[SCSI] Neaten com...|torvalds/linux|\n",
      "|       |2006-02-28 04:55:05|[SCSI] PCI Error ...|torvalds/linux|\n",
      "|    ard|2015-12-13 18:18:29|ARM: only conside...|torvalds/linux|\n",
      "|   Tixy|2011-07-13 17:32:41|ARM: kprobes: Add...|torvalds/linux|\n",
      "|   root|2011-10-28 12:58:57|direct-io: use a ...|torvalds/linux|\n",
      "| Tao Ma|2010-08-12 02:40:01|ocfs2: Add readah...|torvalds/linux|\n",
      "| Wei Xu|2016-04-15 15:43:04|arm64: dts: hikey...|torvalds/linux|\n",
      "| Wei Xu|2014-09-28 02:27:09|clk: hix5hd2: add...|torvalds/linux|\n",
      "| Wei Xu|2014-09-28 02:27:04|clk: hix5hd2: add...|torvalds/linux|\n",
      "|Al Viro|2011-03-23 20:36:55|mm: arch: rename ...|torvalds/linux|\n",
      "|Al Viro|2016-03-31 04:30:15|posix_acl: Inode ...|torvalds/linux|\n",
      "|Al Viro|2015-02-22 16:38:41|VFS: (Scripted) C...|torvalds/linux|\n",
      "|Al Viro|2009-06-24 12:15:24|reiserfs: remove ...|torvalds/linux|\n",
      "|Al Viro|2009-05-09 14:49:40|fs: dcache fix LR...|torvalds/linux|\n",
      "|Al Viro|2013-09-05 20:23:51|afs: use check_su...|torvalds/linux|\n",
      "|Al Viro|2013-09-04 02:50:28|vfs: call d_op->d...|torvalds/linux|\n",
      "|Al Viro|2011-07-26 16:57:09|fs: add missing u...|torvalds/linux|\n",
      "|Al Viro|2012-10-13 00:15:08|vfs: turn do_path...|torvalds/linux|\n",
      "|Al Viro|2011-05-28 05:03:21|fs: block_page_mk...|torvalds/linux|\n",
      "+-------+-------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'SELECT committer.name, committer.date, message, repo_name \\\n",
    "    FROM commits').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb2ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|count|\n",
      "+--------------------+-----+\n",
      "|      Linus Torvalds|65331|\n",
      "|  Greg Kroah-Hartman|57661|\n",
      "|     David S. Miller|54669|\n",
      "|Mauro Carvalho Ch...|23407|\n",
      "|         Ingo Molnar|19120|\n",
      "|    John W. Linville|18882|\n",
      "|          Mark Brown|17526|\n",
      "|     James Bottomley| 9121|\n",
      "|        Takashi Iwai| 8661|\n",
      "|        Russell King| 8648|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'SELECT committer.name, COUNT(1) AS count \\\n",
    "    FROM commits \\\n",
    "    GROUP BY 1 \\\n",
    "    ORDER BY 2 DESC \\\n",
    "    LIMIT 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411874a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_jun_2012 = spark.sql(\n",
    "    'SELECT COUNT(committer.date) \\\n",
    "    FROM commits \\\n",
    "    WHERE committer.date BETWEEN \"2012-06-01\" AND \"2012-06-30\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c1a6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(committer.date AS `date`)=5015)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_jun_2012.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb7faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(2) HashAggregate(keys=[], functions=[count(_gen_alias_91#91)])\n",
      "   +- ShuffleQueryStage 0\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#174]\n",
      "         +- *(1) HashAggregate(keys=[], functions=[partial_count(_gen_alias_91#91)])\n",
      "            +- *(1) Project [committer#0.date AS _gen_alias_91#91]\n",
      "               +- *(1) Filter ((isnotnull(committer#0) AND (committer#0.date >= 1338508800000000)) AND (committer#0.date <= 1341014400000000))\n",
      "                  +- FileScan parquet [committer#0] Batched: false, DataFilters: [isnotnull(committer#0), (committer#0.date >= 1338508800000000), (committer#0.date <= 13410144000..., Format: Parquet, Location: InMemoryFileIndex[gs://insinyur-data/github4], PartitionFilters: [], PushedFilters: [IsNotNull(committer), GreaterThanOrEqual(committer.date,2012-06-01 00:00:00.0), LessThanOrEqual(..., ReadSchema: struct<committer:struct<date:timestamp>>\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[count(_gen_alias_91#91)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#159]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(_gen_alias_91#91)])\n",
      "         +- Project [committer#0.date AS _gen_alias_91#91]\n",
      "            +- Filter ((isnotnull(committer#0) AND (committer#0.date >= 1338508800000000)) AND (committer#0.date <= 1341014400000000))\n",
      "               +- FileScan parquet [committer#0] Batched: false, DataFilters: [isnotnull(committer#0), (committer#0.date >= 1338508800000000), (committer#0.date <= 13410144000..., Format: Parquet, Location: InMemoryFileIndex[gs://insinyur-data/github4], PartitionFilters: [], PushedFilters: [IsNotNull(committer), GreaterThanOrEqual(committer.date,2012-06-01 00:00:00.0), LessThanOrEqual(..., ReadSchema: struct<committer:struct<date:timestamp>>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_jun_2012.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e70a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
